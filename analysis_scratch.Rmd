---
title: "Defund Police Twitter Analysis"
author: "Caitlin Moroney"
date: "7/6/2020"
output: html_document
---

```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(topicmodels)
library(textstem)
```

# Load Data

```{r}
tweets <- read_csv("r_tweets.csv", col_types = "ddTcccdddcllddddccccccccccccccccdcTcdddccdddccldcTcdddccdddcclccccccccccccccldddddTlcccccccdd")
```

# Data Wrangling

## Clean hashtags

```{r}
tweets %>%
  mutate(hashtags = str_replace_all(hashtags, "[,c()\\\"]+", ""),
         mentions_user_id = str_replace_all(mentions_user_id, "[,c()\\\"]+", ""),
         mentions_screen_name = str_replace_all(mentions_screen_name, "[,c()\\\"]+", "")) %>%
  select(-c(geo_coords, coords_coords, bbox_coords)) ->
  tweets
```

## Grab US tweets

```{r}
tweets %>%
  filter(country_code == "US") ->
  us_tweets
```

## Filter out tweets without location

```{r}
tweets %>%
  filter(!is.na(lat)) ->
  tweets
```

## Only distinct US/all tweets

```{r}
distinct_us_tweets <- us_tweets %>%
  distinct(text, .keep_all = TRUE)

distinct_tweets <- tweets %>%
  distinct(text, .keep_all = TRUE)
```

## Remove Portland Police Activity Log account

```{r}
us_tweets %>%
  filter(screen_name != "pdxpolicelog") ->
  us_tweets
``` 

## Normalize text (lemmatize, remove punctuation & stop words)

```{r}
# load stop words from tidytext package
data("stop_words")

us_tweets %>%
  mutate(lemma_text = lemmatize_strings(text)) %>%
  unnest_tokens(word, lemma_text, token = "tweets", strip_url = TRUE) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!word %in% str_remove_all(stop_words$word, "'")) ->
  word_tokens

word_tok_counts <- word_tokens %>%
  count(text, word)

word_tok_counts %>%
  bind_tf_idf(word, text, n) ->
  word_tok_counts
```

# Topic modeling using LDA

```{r}
word_tok_counts %>%
  cast_dtm(text, word, n) ->
  dtm

dtm

lda_model <- LDA(dtm, k = 5)
```

# Obtain top 10 terms per topic

```{r}
topic_word_matrix <- tidy(lda_model, matrix = "beta")
topic_word_matrix

top_terms <- topic_word_matrix %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```

# Plot terms by topic

```{r}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(x = term, y = beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  ylab("Weight") +
  xlab("Term") +
  ggtitle("Term Weights by Topic")
```

# Plot top word frequencies

```{r}
word_tokens %>%
  count(word) ->
  word_counts

top_20_words <- word_counts %>%
  top_n(20, n)

top_20_words %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n, fill = word)) +
  geom_col(show.legend = FALSE) +
  scale_y_log10() +
  xlab("Word") +
  ylab("Count") +
  coord_flip()
```

# Plot top 20 hashtag frequencies

```{r}
us_tweets %>%
  filter(!is.na(hashtags)) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(str_detect(word, "^#")) %>%
  count(word) ->
  hashtag_counts

top_20_hash <- hashtag_counts %>%
  top_n(20, n)

top_20_hash %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n, fill = word)) +
  geom_col(show.legend = FALSE) +
  scale_y_log10() +
  xlab("Hashtag") +
  ylab("Count") +
  coord_flip()
```

# Extract emojis from tweets

```{r}
emoji_codes <- rtweet::emojis
emoji_codes %>%
  mutate(escaped_codes = stringi::stri_escape_unicode(code),
         num_backslash = str_count(escaped_codes, "\\\\")) %>%
  arrange(desc(num_backslash)) ->
  emoji_codes

emoji_pattern <- str_c(paste0("\\Q", emoji_codes$code, "\\E"), collapse = "|")

text_emojis <- c()
for (i in 1:nrow(us_tweets)) {
  tweet <- us_tweets$text[i]
  tweet_emojis <- unlist(str_extract_all(tweet, emoji_pattern))
  tweet_emojis <- str_c(tweet_emojis, collapse = " ")
  text_emojis <- append(text_emojis, tweet_emojis)
}

us_tweets$text_emoji <- text_emojis
```

# Get emoji counts

```{r}
tweet_emoji_tokens <- us_tweets %>%
  unnest_tokens(word, text_emoji, token = "characters", to_lower = FALSE)

emoji_tok_counts <- tweet_emoji_tokens %>%
  count(text, word)

corpus_emoji_counts <- emoji_tok_counts %>%
  count(word)
```

# Grab & plot top 20 emojis

```{r}
top_20_emojis <- corpus_emoji_counts %>%
  top_n(20, n)

top_20_emojis <- top_20_emojis %>%
  inner_join(emoji_codes, by = c("word" = "code"))

top_20_emojis %>%
  mutate(description = reorder(description, n)) %>%
  ggplot(aes(x = description, y = n, fill = description)) +
  geom_col(show.legend = FALSE) +
  xlab("Emoji") +
  ylab("Count") +
  coord_flip()
```

# Plot tweets on US states map

```{r}
# plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = 0.25)

# plot lat & long coords on map
with(us_tweets, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```



